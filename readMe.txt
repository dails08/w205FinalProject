Flat text email files were drawn from the Enron corpus and processed in a number of ways.

We tackled two new (to us) technologies at once and brought them together at the end.  The first was MapReduce/Hadoop which we used to compute across all the emails and calculate when the most emails were sent and who sent the most emails.  The second was NLP using the nltk python library which we combined with MapReduce to calculate the percentage of business emails to personal emails.  We got this to run locally and in a distributed fashion across a subset of the corpus but issues with S3 timing out connections stymied our attempts to run this across the entire corpus. It works, though; mappers complete, reducers begin, but eventually S3 will stop sending data when asked which crashes the mapper and ruins the job.

First was NLP using the nltk python library.  All of the files involved can be found in the sentimentanalysis folder. The goal was to be able to classify an email as business, personal, or neutral.  Two products were necessary, a training set and a feature vector.  The training set was built by extracting the body of each email and submitting a subset of 677 emails to Amazon Mechanical Turk.  For about $20 they came back well-categorized and provided us with a training set.  The result can be found in processedlabels.txt.  Next we needed afeature vector.  We ran this locally using featurevector.py and across all the emails using mr-featurevector.py.  We used this together with the training set to train a classifier which we pickled for later use.  It's not very readable, but you can find it in email_classifier.pickle.  The feature vector, by the way, is an MRJob product and as such all the entries had "" around them which rendered it useless for actual use; all of the emails were getting classified as business.  Once we discovered and corrected this insidious bug, the classifier worked fine.  With this completed, we were able to start running the MR job...with MRJob.  With the lessons learned from the non-NLP MR tasks, we stuck with MRJob, but even that required extensive tweaking.  This started as lots and lots of command line options until they got to be so many and so nuanced that we moved them all to a configuration file and pointed MRJob to that.  This can be found in mrjob.conf and mrjob line commands.  Overall, while we ran into some fatal problems that seemed to be out of our hands (not even the AWS EMR dev forums offered any help and two other posters had reported the same problem with no answers forthcoming), we learned quite a lot about how MR and MRJob work.  

Second was MapReduce/Hadoop.  All of the files involved can be found in the top level folder.  Hadoop, it turns out, is terrible at managing a large number of small files. The large block size (128MB default) and the fact that the namenode holds the entire namespace in memory means that not only is there an upper limit to the number of files that can be stored, Hadoop is also slow at retrieving many small files.  Add on top of that the fact that Hadoop can't take advantage of data locality when pulling files from S3 and the result is an extremely slow process.  In fact, we found that local execution could run much, much faster, on the order of ~50x faster on a subset of text about 800MB large.  To defeat this, originally we tried building a summary file with a list of the zipped files and computing across those, but even when unzipped locally the Hadoop bottleneck was the limiting factor.  Following that, extracting a single text file at a time and computing across that, defeated the Hadoop bottleneck, but extracting one file at a time 13k times (for one person's emails) is extraordinarily inefficient.  Executing locally, extracting all at once vs. one at a time was ~400x faster.  Finally, we caved and reformatted the emails so each was on one line and concatenated all of one person's emails into a single large text file.  Hadoop handled that much better.  Hadoop defaults to splitting text into lines for distribution and it seems like this is possible to alter by writing a custom protocol, but that was too far down the rabbit hole for now, so we adjusted by reformatting all the emails to be on one line.  We also did seme standard NLP preprocessing like removing stopwords, stemming, removing punctuation, converting to lower case, etc.  This is done in the emailLiner.py and emailpreprocessor.py files.  With that, all of the processed text files were loaded onto S3.  Calculating the emails sent and times sent was a challenge.  The times emails were sent was fairly easy as the date is in a constant format for each email and present in all of them, so we were able to run that by passing a python script directly to AWS EMR as a mapper and using Hadoop's built-in aggregate reducer.  For some reason, the task of counting who sent the most emails was more fickle, and so many errors kept popping up with EMR that we eventually just used MRJob which made it much easier.  These files are timeCheckMapper.py and emailcounterMRJOB.py, respectively.  

The results of the computations are stored in the results folder.  Some sample postprocessed emails are available in the ptest folder if you'd like to run similar scripts.
