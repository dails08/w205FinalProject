Flat text email files were drawn from the Enron corpus and processed in a number of ways.

We tackled two new (to us) technologies at once and brought them together at the end.  The first was MapReduce/Hadoop which we used to compute across all the emails and calculate when the most emails were sent and who sent the most emails.  The second was NLP using the nltk python library which we combined with MapReduce to calculate the percentage of business emails to personal emails.  We got this to run locally and in a distributed fashion across a subset of the corpus but issues with S3 timing out connections stymied our attempts to run this across the entire corpus. It works, though; mappers complete, reducers begin, but eventually S3 will stop sending data when asked which crashes the mapper and ruins the job.

First was NLP using the nltk python library.  All of the files involved can be found in the sentimentanalysis folder. The goal was to be able to classify an email as business, personal, or neutral.  Two products were necessary, a training set and a feature vector.  The training set was built by extracting the body of each email and submitting a subset of 677 emails to Amazon Mechanical Turk.  For about $20 they came back well-categorized and provided us with a training set.  The result can be found in processedlabels.txt.  Next we needed afeature vector.  We ran this locally using featurevector.py and across all the emails using mr-featurevector.py.  We used this together with the training set to train a classifier which we pickled for later use.  It's not very readable, but you can find it in email_classifier.pickle.  With this completed, 

Second was MapReduce/Hadoop.  All of the files involved can be found in the top level folder.  
